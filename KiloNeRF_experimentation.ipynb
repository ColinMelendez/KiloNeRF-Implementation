{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "set-up and import the required modules\n"
      ],
      "metadata": {
        "id": "U8yKhrnjSegd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EPvYyKVZLSYu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KiloNeRF model, featuring positional encoding."
      ],
      "metadata": {
        "id": "tFptKNfY3VIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KiloNerf(nn.Module):\n",
        "\n",
        "    def __init__(self, N, embedding_dim_pos=10, embedding_dim_direction=4, scene_scale=3):\n",
        "        super(KiloNerf, self).__init__()\n",
        "\n",
        "        # KiloNerf layers\n",
        "        self.layer1_w = torch.nn.Parameter(torch.zeros((N, N, N, 63, 32)).uniform_(-np.sqrt(6. / 85), np.sqrt(6. / 85)))\n",
        "        self.layer1_b = torch.nn.Parameter(torch.zeros((N, N, N, 1, 32)))\n",
        "        self.layer2_w = torch.nn.Parameter(torch.zeros((N, N, N, 32, 33)).uniform_(-np.sqrt(6. / 64), np.sqrt(6. / 64)))\n",
        "        self.layer2_b = torch.nn.Parameter(torch.zeros((N, N, N, 1, 33)))\n",
        "        self.layer3_w = torch.nn.Parameter(torch.zeros((N, N, N, 32, 32)).uniform_(-np.sqrt(6. / 64), np.sqrt(6. / 64)))\n",
        "        self.layer3_b = torch.nn.Parameter(torch.zeros((N, N, N, 1, 32)))\n",
        "        self.layer4_w = torch.nn.Parameter(\n",
        "            torch.zeros((N, N, N, 27 + 32, 32)).uniform_(-np.sqrt(6. / 64), np.sqrt(6. / 64)))\n",
        "        self.layer4_b = torch.nn.Parameter(torch.zeros((N, N, N, 1, 32)))\n",
        "        self.layer5_w = torch.nn.Parameter(torch.zeros((N, N, N, 32, 3)).uniform_(-np.sqrt(6. / 35), np.sqrt(6. / 35)))\n",
        "        self.layer5_b = torch.nn.Parameter(torch.zeros((N, N, N, 1, 3)))\n",
        "\n",
        "        self.embedding_dim_pos = embedding_dim_pos\n",
        "        self.embedding_dim_direction = embedding_dim_direction\n",
        "        self.N = N\n",
        "        self.scale = scene_scale\n",
        "\n",
        "    @staticmethod\n",
        "    def positional_encoding(x, L):\n",
        "        out = [x]\n",
        "        for j in range(L):\n",
        "            out.append(torch.sin(2 ** j * x))\n",
        "            out.append(torch.cos(2 ** j * x))\n",
        "        return torch.cat(out, dim=1)\n",
        "\n",
        "    def forward(self, x, d):\n",
        "        color = torch.zeros_like(x)\n",
        "        sigma = torch.zeros((x.shape[0]), device=x.device)\n",
        "\n",
        "        mask = (x[:, 0].abs() < (self.scale / 2)) & (x[:, 1].abs() < (self.scale / 2)) & (\n",
        "                x[:, 2].abs() < (self.scale / 2))\n",
        "        idx = (x[mask] / (self.scale / self.N) + self.N / 2).long().clip(0, self.N - 1)\n",
        "\n",
        "        emb_x = self.positional_encoding(x[mask], self.embedding_dim_pos)\n",
        "        emb_d = self.positional_encoding(d[mask], self.embedding_dim_direction)\n",
        "\n",
        "        # Implementation of the MLP architecture\n",
        "        h = torch.relu(emb_x.unsqueeze(1) @ self.layer1_w[idx[:, 0], idx[:, 1], idx[:, 2]] + \\\n",
        "                       self.layer1_b[idx[:, 0], idx[:, 1], idx[:, 2]])\n",
        "        h = torch.relu(h @ self.layer2_w[idx[:, 0], idx[:, 1], idx[:, 2]] + self.layer2_b[idx[:, 0], idx[:, 1],\n",
        "                                                                                          idx[:, 2]])\n",
        "        h, density = h[:, :, :-1], h[:, :, -1]\n",
        "        h = h @ self.layer3_w[idx[:, 0], idx[:, 1], idx[:, 2]] + self.layer3_b[idx[:, 0], idx[:, 1], idx[:, 2]]\n",
        "        h = torch.relu(torch.cat((h, emb_d.unsqueeze(1)), dim=-1) @ self.layer4_w[idx[:, 0], idx[:, 1], idx[:, 2]] + \\\n",
        "                       self.layer4_b[idx[:, 0], idx[:, 1], idx[:, 2]])\n",
        "        c = torch.sigmoid(h @ self.layer5_w[idx[:, 0], idx[:, 1], idx[:, 2]] + self.layer5_b[idx[:, 0], idx[:, 1],\n",
        "                                                                                             idx[:, 2]])\n",
        "        color[mask] = c.squeeze(1)\n",
        "        sigma[mask] = density.squeeze(1)\n",
        "        return color, sigma\n",
        "\n"
      ],
      "metadata": {
        "id": "a0rBDJoTTULn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the functions for rendering rays with the model"
      ],
      "metadata": {
        "id": "u5OYtECR3KbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accumulated_transmittance(alphas):\n",
        "    accumulated_transmittance = torch.cumprod(alphas, 1)\n",
        "    return torch.cat((torch.ones((accumulated_transmittance.shape[0], 1), device=alphas.device),\n",
        "                      accumulated_transmittance[:, :-1]), dim=-1)\n",
        "\n",
        "def render_rays(nerf_model, ray_origins, ray_directions, hn=0, hf=0.5, nb_bins=192):\n",
        "    device = ray_origins.device\n",
        "    t = torch.linspace(hn, hf, nb_bins, device=device).expand(ray_origins.shape[0], nb_bins)\n",
        "    # Perturb sampling along each ray.\n",
        "    mid = (t[:, :-1] + t[:, 1:]) / 2.\n",
        "    lower = torch.cat((t[:, :1], mid), -1)\n",
        "    upper = torch.cat((mid, t[:, -1:]), -1)\n",
        "    u = torch.rand(t.shape, device=device)\n",
        "    t = lower + (upper - lower) * u  # [batch_size, nb_bins]\n",
        "    delta = torch.cat((t[:, 1:] - t[:, :-1], torch.tensor([1e10], device=device).expand(ray_origins.shape[0], 1)), -1)\n",
        "\n",
        "    x = ray_origins.unsqueeze(1) + t.unsqueeze(2) * ray_directions.unsqueeze(1)  # [batch_size, nb_bins, 3]\n",
        "    ray_directions = ray_directions.expand(nb_bins, ray_directions.shape[0], 3).transpose(0, 1)\n",
        "\n",
        "    colors, sigma = nerf_model(x.reshape(-1, 3), ray_directions.reshape(-1, 3))\n",
        "    colors = colors.reshape(x.shape)\n",
        "    sigma = sigma.reshape(x.shape[:-1])\n",
        "\n",
        "    alpha = 1 - torch.exp(-sigma * delta)  # [batch_size, nb_bins]\n",
        "    weights = compute_accumulated_transmittance(1 - alpha).unsqueeze(2) * alpha.unsqueeze(2)\n",
        "    c = (weights * colors).sum(dim=1)  # Pixel values\n",
        "    # Regularization for white background\n",
        "    weight_sum = weights.sum(-1).sum(-1)\n",
        "    return c + 1 - weight_sum.unsqueeze(-1)"
      ],
      "metadata": {
        "id": "O7l326ppTs_r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to generate images from the model"
      ],
      "metadata": {
        "id": "zpbh4YF93Crw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def generate_view(hn, hf, dataset, chunk_size=5, img_index=0, nb_bins=192, H=400, W=400):\n",
        "    ray_origins = dataset[img_index * H * W: (img_index + 1) * H * W, :3]\n",
        "    ray_directions = dataset[img_index * H * W: (img_index + 1) * H * W, 3:6]\n",
        "\n",
        "    data = []\n",
        "    for i in range(int(np.ceil(H / chunk_size))):\n",
        "        ray_origins_ = ray_origins[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
        "        ray_directions_ = ray_directions[i * W * chunk_size: (i + 1) * W * chunk_size].to(device)\n",
        "        regenerated_px_values = render_rays(model, ray_origins_, ray_directions_, hn=hn, hf=hf, nb_bins=nb_bins)\n",
        "        data.append(regenerated_px_values)\n",
        "    img = torch.cat(data).data.cpu().numpy().reshape(H, W, 3)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f'novel_views/img_{img_index}.png', bbox_inches='tight')\n",
        "    plt.close()\n"
      ],
      "metadata": {
        "id": "09XucR24Sn-B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the training function"
      ],
      "metadata": {
        "id": "Fn9fNkP640t6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(nerf_model, optimizer, scheduler, data_loader, device='cpu', hn=0, hf=1, nb_epochs=int(1e5), nb_bins=192):\n",
        "    training_loss = []\n",
        "    for _ in (range(nb_epochs)):\n",
        "        for ep, batch in enumerate(tqdm(data_loader)):\n",
        "            ray_origins = batch[:, :3].to(device)\n",
        "            ray_directions = batch[:, 3:6].to(device)\n",
        "            ground_truth_px_values = batch[:, 6:].to(device)\n",
        "\n",
        "            regenerated_px_values = render_rays(nerf_model, ray_origins, ray_directions, hn=hn, hf=hf, nb_bins=nb_bins)\n",
        "            loss = ((ground_truth_px_values - regenerated_px_values) ** 2).sum()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            training_loss.append(loss.item())\n",
        "        scheduler.step()\n",
        "    return training_loss\n"
      ],
      "metadata": {
        "id": "dKnT0b5ZT2xL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the the training environment"
      ],
      "metadata": {
        "id": "_xVayRBS33rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "num_epochs = 8\n",
        "training_dataset = torch.from_numpy(np.load('training_data.pkl', allow_pickle=True))\n",
        "testing_dataset = torch.from_numpy(np.load('testing_data.pkl', allow_pickle=True))\n",
        "model = KiloNerf(16).to(device)\n",
        "model_optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.MultiStepLR(model_optimizer, milestones=[2, 4, 6], gamma=0.5)\n",
        "data_loader = DataLoader(training_dataset, batch_size=2048, shuffle=True)"
      ],
      "metadata": {
        "id": "qQzamkrWUG-W"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "yguz_rYc4K4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train(model, model_optimizer, scheduler, data_loader, nb_epochs=num_epochs, device=device, hn=2, hf=6, nb_bins=192)\n"
      ],
      "metadata": {
        "id": "RUVml2EoT5_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "genreate view images from the trained model."
      ],
      "metadata": {
        "id": "4pGFpRsL4NGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(200):\n",
        "    generate_view(2, 6, testing_dataset, img_index=idx, nb_bins=192, H=400, W=400)"
      ],
      "metadata": {
        "id": "5RwZnchzyzvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}